# Deduper Notes

## Clipcig() and Segmentation attempt
Initially attempted to parse through file in segments of 25000, and replace the oldest record to maintain only 25,000 unique IDs in memory.
This method ended up taking substantially longer (1hr+) while producing over ~1 million excess "good reads" when compared to the consensus. (13,719,048) 

This attempt utilized more stepwise regex, and list manipulation, as well as loading the read's sequence into the unique ID str object generated by concatenating
relevant sam fields. In addition, a workaround was necessary to avoid "index out of range" for records on the negative strand, which only had clipping on the right side of the cigar string (implemented a conditional pass for empty regex match list at certain indexes, which probably bricked the functions selectivity.Therefore, it was necessary to revamp clipcig, and store our unique read parameters in a different way. 

![image](https://user-images.githubusercontent.com/106117735/200739957-a9f2b43d-ac43-4338-93c3-aa87ffa0a938.png)

## Clipcig2()+cigarmutate() and Dictionary Attempt
As mentioned in lectures prior, dictionaries are great for dealing with big files which may contain repeated records/parameters. However, there was clearly something
wrong with our position adjusting function (clipcig), making it less constringent. Clipcig2 simplifies position adjustment by working in tandem with cigarmutate's output.
(list of regex matches based on letter in cigar string) This makes the math easier, and identifying cigar consituents more standardized as they all belong to the same 
regex match object. 

![image](https://user-images.githubusercontent.com/106117735/200740019-aa60de50-8652-41b9-8c91-d24311d038c9.png)

The dictionary, alongside the simplified clipcig2 function enhanced the speed tremendously. (~2min completion) And produced results within ~10,000 of the consensus for
non-duplicate reads.
